{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6337743-2e1a-4c9a-8352-2ddfa0db3a51",
   "metadata": {},
   "source": [
    "### Importamos ciertas librerias necesarias para realizar scraping con respecto a los productos en la RD que se encuentran en el dataset de Open Food Facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3898dbe1-c73d-4a46-b6dd-aa0270644525",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67546ca-72d1-40ba-9936-96bd31eef889",
   "metadata": {},
   "source": [
    "### Realizamos el scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688cb2f8-f808-417d-a811-5157ead0649d",
   "metadata": {},
   "source": [
    "## Prueba del modelo sin entrenamiento previo\n",
    "\n",
    "### Importaciones y configuración global\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f185294-633a-4b9d-ab7f-cd2bb5f715c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, recall_score, precision_score, f1_score, accuracy_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from IPython.display import display, Image as IPImage\n",
    "\n",
    "# Configuraciones globales\n",
    "BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "MODEL_NAME = \"llama3.2-vision\"\n",
    "VALID_CLASSES = ['agua', 'leche', 'galletas']\n",
    "BRANDS = {\n",
    "    'agua': ['planeta azul', 'dasani', 'cascada'],\n",
    "    'leche': ['carnation', 'rica', 'nestlé', 'milex'],\n",
    "    'galletas': ['club social', 'oreo', 'club extra', 'guarina']\n",
    "}\n",
    "ERROR_CLASS = 'desconocido'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3b5c25-b1db-4dc6-9aab-1293d9d8db1a",
   "metadata": {},
   "source": [
    "### Funciones de procesamiento de imágenes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9289cc19-35e0-49ef-8dc7-7a993db7bbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_image(image_path):\n",
    "    \"\"\"Codifica una imagen a base64 con manejo mejorado de formatos.\"\"\"\n",
    "    try:\n",
    "        if isinstance(image_path, str):\n",
    "            with Image.open(image_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format='JPEG')\n",
    "                img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        else:\n",
    "            img = image_path\n",
    "            if img.mode != 'RGB':\n",
    "                img = img.convert('RGB')\n",
    "            buffer = io.BytesIO()\n",
    "            img.save(buffer, format='JPEG')\n",
    "            img_str = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        \n",
    "        return img_str\n",
    "    except Exception as e:\n",
    "        print(f\"Error al codificar la imagen: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def clean_prediction(prediction: str) -> Tuple[str, str]:\n",
    "    \"\"\"Limpia la respuesta del modelo para extraer la categoría y la marca.\"\"\"\n",
    "    prediction_lower = prediction.lower()\n",
    "    \n",
    "    # Primero identificamos la categoría\n",
    "    category_scores = {clase: 0 for clase in VALID_CLASSES}\n",
    "    \n",
    "    # Palabras clave para categorías\n",
    "    keywords = {\n",
    "        'agua': ['agua', 'botella', 'bebida', 'refresco', 'líquido'],\n",
    "        'leche': ['leche', 'lácteo', 'dairy', 'milk', 'evaporada', 'condensada'],\n",
    "        'galletas': ['galleta', 'cookie', 'oreo', 'snack', 'dulce', 'club social', 'cracker', 'sandwich']\n",
    "    }\n",
    "    \n",
    "    # Evaluar categoría\n",
    "    for categoria, palabras in keywords.items():\n",
    "        for palabra in palabras:\n",
    "            if palabra in prediction_lower:\n",
    "                category_scores[categoria] += 1\n",
    "    \n",
    "    # Determinar categoría\n",
    "    category = max(category_scores.items(), key=lambda x: x[1])[0] if max(category_scores.values()) > 0 else VALID_CLASSES[0]\n",
    "    \n",
    "    # Buscar marca\n",
    "    brand = \"marca desconocida\"\n",
    "    for marca in BRANDS[category]:\n",
    "        if marca.lower() in prediction_lower:\n",
    "            brand = marca\n",
    "            break\n",
    "            \n",
    "    if brand == \"marca desconocida\":\n",
    "        common_prefixes = [\"marca\", \"producto\", \"de la marca\", \"elaborado por\"]\n",
    "        for prefix in common_prefixes:\n",
    "            if prefix in prediction_lower:\n",
    "                start_idx = prediction_lower.find(prefix) + len(prefix)\n",
    "                end_idx = prediction_lower.find(\" \", start_idx + 15) if \" \" in prediction_lower[start_idx:] else len(prediction_lower)\n",
    "                potential_brand = prediction_lower[start_idx:end_idx].strip()\n",
    "                if len(potential_brand) > 2:\n",
    "                    brand = potential_brand\n",
    "                    break\n",
    "    \n",
    "    return category, brand\n",
    "\n",
    "def analyze_image(image_path: str, prompt: str) -> str:\n",
    "    \"\"\"Analiza una imagen usando el modelo de visión.\"\"\"\n",
    "    base64_image = encode_image(image_path)\n",
    "    if not base64_image:\n",
    "        return f\"{ERROR_CLASS} (marca desconocida)\"\n",
    "    \n",
    "    payload = {\n",
    "        \"model\": MODEL_NAME,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"images\": [base64_image]\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        print(\"Enviando petición a Ollama...\")\n",
    "        response = requests.post(BASE_URL, json=payload)\n",
    "        print(f\"Código de estado: {response.status_code}\")\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            prediction = result.get('response', 'No se pudo obtener una respuesta')\n",
    "            category, brand = clean_prediction(prediction)\n",
    "            print(f\"Predicción original: {prediction}\")\n",
    "            print(f\"Categoría: {category}\")\n",
    "            print(f\"Marca: {brand}\")\n",
    "            return f\"{category} ({brand})\"\n",
    "        else:\n",
    "            print(f\"Respuesta completa del servidor: {response.text}\")\n",
    "            return f\"{ERROR_CLASS} (marca desconocida)\"\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error detallado: {str(e)}\")\n",
    "        return f\"{ERROR_CLASS} (marca desconocida)\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61082d0c-e806-4678-8ef3-a6736718072e",
   "metadata": {},
   "source": [
    "### Funciones de métricas y visualización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53dd5c70-3d79-4aa9-9cca-9db1f25d2416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(predictions: List[str], ground_truth: List[str]) -> Dict[str, float]:\n",
    "    \"\"\"Calcula métricas de evaluación.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Limpiar predicciones para obtener solo la categoría (antes del paréntesis)\n",
    "    cleaned_predictions = [pred.split('(')[0].strip() for pred in predictions]\n",
    "    \n",
    "    y_true = le.fit_transform(ground_truth)\n",
    "    y_pred = le.transform(cleaned_predictions)\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision_macro': precision_score(y_true, y_pred, average='macro'),\n",
    "        'recall_macro': recall_score(y_true, y_pred, average='macro'),\n",
    "        'f1_macro': f1_score(y_true, y_pred, average='macro')\n",
    "    }\n",
    "    \n",
    "    classes = le.classes_\n",
    "    for i, class_name in enumerate(classes):\n",
    "        y_true_binary = (y_true == i)\n",
    "        y_pred_binary = (y_pred == i)\n",
    "        \n",
    "        metrics[f'precision_{class_name}'] = precision_score(y_true_binary, y_pred_binary)\n",
    "        metrics[f'recall_{class_name}'] = recall_score(y_true_binary, y_pred_binary)\n",
    "        metrics[f'f1_{class_name}'] = f1_score(y_true_binary, y_pred_binary)\n",
    "        \n",
    "    return metrics\n",
    "\n",
    "def plot_confusion_matrix(predictions: List[str], ground_truth: List[str]) -> None:\n",
    "    \"\"\"Genera y muestra la matriz de confusión.\"\"\"\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Limpiar predicciones para obtener solo la categoría\n",
    "    cleaned_predictions = [pred.split('(')[0].strip() for pred in predictions]\n",
    "    \n",
    "    y_true = le.fit_transform(ground_truth)\n",
    "    y_pred = le.transform(cleaned_predictions)\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, \n",
    "               annot=True, \n",
    "               fmt='d', \n",
    "               cmap='Blues',\n",
    "               xticklabels=le.classes_,\n",
    "               yticklabels=le.classes_)\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.xlabel('Predicción')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c67b79-8737-4378-a872-a3aac59bc3c7",
   "metadata": {},
   "source": [
    "### Función principal de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a74c59f6-3f53-46e7-9934-310ed8503603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_vision_with_metrics(image_paths: List[str], \n",
    "                           ground_truth: List[str] = None, \n",
    "                           prompt: str = None) -> Dict[str, float]:\n",
    "    \"\"\"Función de prueba que evalúa múltiples imágenes y muestra métricas.\"\"\"\n",
    "    \n",
    "    if ground_truth is None:\n",
    "        ground_truth = ['galletas'] * len(image_paths)\n",
    "    \n",
    "    if len(image_paths) != len(ground_truth):\n",
    "        print(\"ADVERTENCIA: El número de imágenes no coincide con el número de etiquetas ground truth\")\n",
    "        if len(image_paths) > len(ground_truth):\n",
    "            ground_truth.extend([ground_truth[-1]] * (len(image_paths) - len(ground_truth)))\n",
    "        else:\n",
    "            ground_truth = ground_truth[:len(image_paths)]\n",
    "    \n",
    "    if prompt is None:\n",
    "        prompt = f\"Identifica qué tipo de producto ves en esta imagen y su marca. El producto debe ser uno de estos: {', '.join(VALID_CLASSES)}. Menciona explícitamente tanto el tipo de producto como la marca, especialmente si es una marca dominicana como Planeta Azul, Club Social, Rica, etc.\"\n",
    "    \n",
    "    print(\"Ejemplos de imágenes a analizar:\")\n",
    "    for i, path in enumerate(image_paths):\n",
    "        try:\n",
    "            display(IPImage(filename=path))\n",
    "            print(f\"Ground truth: {ground_truth[i]}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error al mostrar la imagen {path}: {str(e)}\")\n",
    "    \n",
    "    print(\"\\nProcesando imágenes...\")\n",
    "    predictions = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        print(f\"\\nProcesando imagen {i+1}/{len(image_paths)}: {image_path}\")\n",
    "        prediction = analyze_image(image_path, prompt)\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    print(\"\\nResumen de predicciones:\")\n",
    "    for i, (pred, true) in enumerate(zip(predictions, ground_truth)):\n",
    "        print(f\"Imagen {i+1}: Predicción = {pred}, Ground Truth = {true}\")\n",
    "    \n",
    "    metrics = calculate_metrics(predictions, ground_truth)\n",
    "    plot_confusion_matrix(predictions, ground_truth)\n",
    "    \n",
    "    print(\"\\nMétricas de evaluación:\")\n",
    "    for metric_name, value in metrics.items():\n",
    "        print(f\"{metric_name}: {value:.4f}\")\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Celda 5: Ejecución de prueba\n",
    "image_paths = ['37_Planeta Azul.jpg', '197_leche.jpg', '4_Club Social Integral Tradicion.jpg', '7_Oreo Original.jpg', '127_Carnation Evaporated Milk.jpg']\n",
    "ground_truth = ['agua', 'leche', 'galletas', 'galletas', 'leche']\n",
    "metrics = test_vision_with_metrics(image_paths, ground_truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fc6b29-648d-43c6-ada6-8bac2cafdf49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44ea1e5-564f-4cb3-b742-dad1d3540ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6450fef2-996b-47d4-8d12-bf6ca9adc2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Agua dasani.jpg'  'Club Soda.jpg'   lemon.jpg  'Planeta Azul.jpg'\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "68d2d86c-33cc-4be1-8548-2acbcc984840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'dataset/'\n",
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/water\n"
     ]
    }
   ],
   "source": [
    "cd dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "582c4829-0fba-4f75-8c4c-a0513b31049b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train\n"
     ]
    }
   ],
   "source": [
    "cd train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb46ec31-5348-448f-a3b1-4c6336dc6017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f18240ed-2c87-40e9-84fc-6c42f88b3965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/biscuits\n"
     ]
    }
   ],
   "source": [
    "cd biscuits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745fd88f-afcf-471b-a685-6117713d31ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2082f03-b75d-4e65-a960-51c3f2f83858",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/canned\n"
     ]
    }
   ],
   "source": [
    "cd canned/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0568b1e-0aae-476f-96be-40ca1d4a86db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc4a8f68-42a4-40b5-839a-188d3d9c3eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/cereals\n"
     ]
    }
   ],
   "source": [
    "cd cereals/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99fd2ff8-7c9d-4e77-8958-571bd0199d79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00223461-4e72-424d-9766-e96f92a9dfff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/dried-foods\n"
     ]
    }
   ],
   "source": [
    "cd dried-foods/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a237b362-c427-481e-acf3-a00955c928d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "829b4fb5-eb65-49e0-97d9-d09906227fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/milk-powder\n"
     ]
    }
   ],
   "source": [
    "cd milk-powder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01e5fd24-29e3-4b0d-b9d1-8431f9fc9636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc4c6d1c-7b85-4665-91cb-4a03e94d3477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images/water\n"
     ]
    }
   ],
   "source": [
    "cd water/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90db5e6b-0f2e-48ad-9ba4-67755763e70e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93966363-9561-43a6-9783-4ad91fbf41a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6d2916-5795-4719-a2fc-b68634cc0b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5e6b6b-0309-42b6-967f-a5a3ad37e0ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54b181-4ff4-4714-9717-0c0db667dc74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8b1a92df-642d-4f30-973d-52c7d75050f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorías detectadas: ['entrenamiento', 'imagenes_productos', 'train', 'train (Copy)']\n",
      "Iniciando entrenamiento inicial...\n",
      "Iniciando entrenamiento...\n",
      "Preparando datos desde: dataset\n",
      "Found 333 images belonging to 4 classes.\n",
      "Found 82 images belonging to 4 classes.\n",
      "Clases en el generador de entrenamiento: {'entrenamiento': 0, 'imagenes_productos': 1, 'train': 2, 'train (Copy)': 3}\n",
      "Número de clases: 4\n",
      "Tamaño del batch: 32\n",
      "Número de muestras de entrenamiento: 333\n",
      "Número de muestras de validación: 82\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 1s/step - accuracy: 0.2218 - loss: 1.4767 - val_accuracy: 0.3537 - val_loss: 1.4046\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 895ms/step - accuracy: 0.3344 - loss: 1.3724 - val_accuracy: 0.3537 - val_loss: 1.3820\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 990ms/step - accuracy: 0.3220 - loss: 1.4059 - val_accuracy: 0.2317 - val_loss: 1.3979\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.2631 - loss: 1.3920 - val_accuracy: 0.3537 - val_loss: 1.4018\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3238 - loss: 1.4095 - val_accuracy: 0.2317 - val_loss: 1.3785\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 959ms/step - accuracy: 0.3461 - loss: 1.3772 - val_accuracy: 0.3537 - val_loss: 1.3623\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 972ms/step - accuracy: 0.3665 - loss: 1.3551 - val_accuracy: 0.3537 - val_loss: 1.3708\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 1s/step - accuracy: 0.3257 - loss: 1.3805 - val_accuracy: 0.3537 - val_loss: 1.3662\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 962ms/step - accuracy: 0.3515 - loss: 1.3708 - val_accuracy: 0.3537 - val_loss: 1.3805\n",
      "Iniciando fine-tuning...\n",
      "Iniciando fine-tuning...\n",
      "Iniciando entrenamiento...\n",
      "Preparando datos desde: dataset\n",
      "Found 333 images belonging to 4 classes.\n",
      "Found 82 images belonging to 4 classes.\n",
      "Clases en el generador de entrenamiento: {'entrenamiento': 0, 'imagenes_productos': 1, 'train': 2, 'train (Copy)': 3}\n",
      "Número de clases: 4\n",
      "Tamaño del batch: 32\n",
      "Número de muestras de entrenamiento: 333\n",
      "Número de muestras de validación: 82\n",
      "Epoch 1/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 1s/step - accuracy: 0.3467 - loss: 1.3966 - val_accuracy: 0.3537 - val_loss: 1.3621\n",
      "Epoch 2/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3354 - loss: 1.4291 - val_accuracy: 0.3537 - val_loss: 1.3625\n",
      "Epoch 3/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3323 - loss: 1.3935 - val_accuracy: 0.3537 - val_loss: 1.3628\n",
      "Epoch 4/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3307 - loss: 1.3825 - val_accuracy: 0.3537 - val_loss: 1.3632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class ProductClassifier:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.img_size = (224, 224)\n",
    "        # Detectar categorías automáticamente del directorio\n",
    "        self.categories = self._detect_categories()\n",
    "        print(f\"Categorías detectadas: {self.categories}\")\n",
    "        \n",
    "        # Modelo para clasificación de categorías\n",
    "        self.category_model = self._build_category_model()\n",
    "        \n",
    "    def _detect_categories(self):\n",
    "        \"\"\"Detecta automáticamente las categorías presentes en el directorio\"\"\"\n",
    "        categories = [d for d in os.listdir(self.base_path) \n",
    "                     if os.path.isdir(os.path.join(self.base_path, d))]\n",
    "        return sorted(categories)  # Ordenar para consistencia\n",
    "            \n",
    "    def _build_category_model(self):\n",
    "        \"\"\"Construye el modelo base usando EfficientNetB0 pre-entrenado\"\"\"\n",
    "        base_model = EfficientNetB0(weights='imagenet', include_top=False)\n",
    "        \n",
    "        # Congelar capas base\n",
    "        for layer in base_model.layers:\n",
    "            layer.trainable = False\n",
    "            \n",
    "        # Agregar capas de clasificación\n",
    "        x = GlobalAveragePooling2D()(base_model.output)\n",
    "        x = Dense(256, activation='relu')(x)\n",
    "        outputs = Dense(len(self.categories), activation='softmax')(x)\n",
    "        \n",
    "        model = Model(inputs=base_model.input, outputs=outputs)\n",
    "        model.compile(optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     metrics=['accuracy'])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Prepara los generadores de datos para entrenamiento y validación\"\"\"\n",
    "        train_datagen = ImageDataGenerator(\n",
    "            rescale=1./255,\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.2,\n",
    "            height_shift_range=0.2,\n",
    "            horizontal_flip=True,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "        \n",
    "        print(f\"Preparando datos desde: {self.base_path}\")\n",
    "        \n",
    "        train_generator = train_datagen.flow_from_directory(\n",
    "            self.base_path,\n",
    "            target_size=self.img_size,\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='training',\n",
    "            classes=self.categories,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        validation_generator = train_datagen.flow_from_directory(\n",
    "            self.base_path,\n",
    "            target_size=self.img_size,\n",
    "            batch_size=32,\n",
    "            class_mode='categorical',\n",
    "            subset='validation',\n",
    "            classes=self.categories,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        return train_generator, validation_generator\n",
    "    \n",
    "    def train(self, epochs=10):\n",
    "        \"\"\"Entrena el modelo con los datos preparados\"\"\"\n",
    "        print(\"Iniciando entrenamiento...\")\n",
    "        train_generator, validation_generator = self.prepare_data()\n",
    "        \n",
    "        # Imprimir información sobre los generadores\n",
    "        print(f\"Clases en el generador de entrenamiento: {train_generator.class_indices}\")\n",
    "        print(f\"Número de clases: {len(train_generator.class_indices)}\")\n",
    "        print(f\"Tamaño del batch: {train_generator.batch_size}\")\n",
    "        print(f\"Número de muestras de entrenamiento: {train_generator.samples}\")\n",
    "        print(f\"Número de muestras de validación: {validation_generator.samples}\")\n",
    "        \n",
    "        history = self.category_model.fit(\n",
    "            train_generator,\n",
    "            validation_data=validation_generator,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    patience=3,\n",
    "                    restore_best_weights=True\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def fine_tune(self, epochs=5):\n",
    "        \"\"\"Fine-tune el modelo desbloqueando algunas capas\"\"\"\n",
    "        print(\"Iniciando fine-tuning...\")\n",
    "        # Desbloquear últimas capas del modelo base\n",
    "        for layer in self.category_model.layers[-20:]:\n",
    "            layer.trainable = True\n",
    "            \n",
    "        self.category_model.compile(\n",
    "            optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "            loss='categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return self.train(epochs)\n",
    "    \n",
    "    def predict_category(self, image_path):\n",
    "        \"\"\"Predice la categoría de una imagen\"\"\"\n",
    "        img = tf.keras.preprocessing.image.load_img(\n",
    "            image_path, target_size=self.img_size\n",
    "        )\n",
    "        img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "        img_array = np.expand_dims(img_array, axis=0)\n",
    "        img_array /= 255.\n",
    "        \n",
    "        predictions = self.category_model.predict(img_array)\n",
    "        predicted_category = self.categories[np.argmax(predictions[0])]\n",
    "        confidence = np.max(predictions[0])\n",
    "        \n",
    "        return predicted_category, confidence\n",
    "    \n",
    "    def save_models(self, path):\n",
    "        \"\"\"Guarda los modelos entrenados\"\"\"\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        self.category_model.save(os.path.join(path, 'category_model.h5'))\n",
    "    \n",
    "    def load_models(self, path):\n",
    "        \"\"\"Carga modelos previamente entrenados\"\"\"\n",
    "        self.category_model = tf.keras.models.load_model(\n",
    "            os.path.join(path, 'category_model.h5')\n",
    "        )\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"dataset\"\n",
    "    classifier = ProductClassifier(base_path)\n",
    "    \n",
    "    # Entrenamiento inicial\n",
    "    print(\"Iniciando entrenamiento inicial...\")\n",
    "    history = classifier.train(epochs=10)\n",
    "    \n",
    "    # Fine-tuning\n",
    "    print(\"Iniciando fine-tuning...\")\n",
    "    history_ft = classifier.fine_tune(epochs=5)\n",
    "    \n",
    "    # Guardar modelos\n",
    "    classifier.save_models(\"./models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d9afd7-1627-45b1-a4b1-534d6db2cdfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "518bb87a-5017-4e2b-89b5-0c4be6b45d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Categoría predicha: imagenes_productos (Confianza: 0.35)\n"
     ]
    }
   ],
   "source": [
    "    # Ejemplo de predicción\n",
    "    test_image = \"7_Oreo Original.jpg\"\n",
    "    category, confidence = classifier.predict_category(test_image)\n",
    "    print(f\"Categoría predicha: {category} (Confianza: {confidence:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "adf5f28f-ad8f-455d-b80b-b50d4c3b3147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.26.0'                                ej3.jpeg\n",
      "'127_Carnation Evaporated Milk.jpg'      ej4.jpeg\n",
      " 197_leche.jpg                           forClaudeContex.ipynb\n",
      "'37_Planeta Azul.jpg'                    \u001b[0m\u001b[01;34mimagenes_productos\u001b[0m/\n",
      "'4_Club Social Integral Tradicion.jpg'   \u001b[01;34mmodels\u001b[0m/\n",
      "'7_Oreo Original.jpg'                    output.jpg\n",
      " catalog_metadata.json                   productos_rd.csv\n",
      " \u001b[01;34mdataset\u001b[0m/                                Untitled.ipynb\n",
      " \u001b[01;34mdataset2.0\u001b[0m/                             yolov8n.pt\n",
      " ej1.jpeg                                yolov8x.pt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "97344f80-11f4-4b6a-9eb5-ba165a7b520f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f8ba7b14-93c2-46f0-8860-b038cf76853d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset\n"
     ]
    }
   ],
   "source": [
    "cd dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "999ae125-b192-426e-a678-c021cf85a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from PIL import Image\n",
    "import json\n",
    "\n",
    "class DataProcessor:\n",
    "    def __init__(self, base_path):\n",
    "        self.base_path = base_path\n",
    "        self.categories = ['biscuits', 'canned', 'cereals', 'dried-foods', 'milk-powder', 'water']\n",
    "        self.metadata = {}\n",
    "        \n",
    "    def process_dataset(self):\n",
    "        \"\"\"Procesa el dataset y genera metadata\"\"\"\n",
    "        for category in self.categories:\n",
    "            category_path = os.path.join(self.base_path, category)\n",
    "            if not os.path.exists(category_path):\n",
    "                continue\n",
    "                \n",
    "            self.metadata[category] = {\n",
    "                'products': [],\n",
    "                'total_images': 0,\n",
    "                'brands': set()\n",
    "            }\n",
    "            \n",
    "            for image_name in os.listdir(category_path):\n",
    "                if image_name.endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    product_name = self._clean_product_name(image_name)\n",
    "                    brand = self._extract_brand(product_name)\n",
    "                    \n",
    "                    self.metadata[category]['products'].append({\n",
    "                        'filename': image_name,\n",
    "                        'product_name': product_name,\n",
    "                        'brand': brand\n",
    "                    })\n",
    "                    self.metadata[category]['total_images'] += 1\n",
    "                    self.metadata[category]['brands'].add(brand)\n",
    "            \n",
    "            # Convertir set a lista para serialización JSON\n",
    "            self.metadata[category]['brands'] = list(self.metadata[category]['brands'])\n",
    "    \n",
    "    def _clean_product_name(self, filename):\n",
    "        \"\"\"Limpia el nombre del producto del nombre del archivo\"\"\"\n",
    "        name = os.path.splitext(filename)[0]\n",
    "        # Eliminar prefijos comunes como \"WhatsApp Image\" etc.\n",
    "        if \"WhatsApp Image\" in name:\n",
    "            name = \"Unknown Product\"\n",
    "        return name.replace('.', ' ').strip()\n",
    "    \n",
    "    def _extract_brand(self, product_name):\n",
    "        \"\"\"Extrae la marca del nombre del producto\"\"\"\n",
    "        # Lista de marcas conocidas\n",
    "        known_brands = {\n",
    "            'biscuits': ['Casino', 'Club Social', 'Oreo', 'Ritz', 'Costa', 'Emperador'],\n",
    "            'water': ['Planeta Azul', 'Dasani'],\n",
    "            'milk-powder': ['Milex', 'Nido'],\n",
    "            'cereals': ['Nesquik', 'Corn Flakes', 'Trix'],\n",
    "            'canned': ['Goya', 'La Famosa'],\n",
    "            'dried-foods': ['Nesquik', 'Sustagen']\n",
    "        }\n",
    "        \n",
    "        for category, brands in known_brands.items():\n",
    "            for brand in brands:\n",
    "                if brand.lower() in product_name.lower():\n",
    "                    return brand\n",
    "        return \"Unknown Brand\"\n",
    "    \n",
    "    def generate_dataset_summary(self):\n",
    "        \"\"\"Genera un resumen del dataset\"\"\"\n",
    "        summary = {\n",
    "            'total_categories': len(self.categories),\n",
    "            'total_images': sum(meta['total_images'] for meta in self.metadata.values()),\n",
    "            'categories': self.metadata\n",
    "        }\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def save_metadata(self, output_path):\n",
    "        \"\"\"Guarda la metadata en formato JSON\"\"\"\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.metadata, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    def validate_images(self):\n",
    "        \"\"\"Valida la integridad de las imágenes\"\"\"\n",
    "        invalid_images = []\n",
    "        \n",
    "        for category in self.categories:\n",
    "            category_path = os.path.join(self.base_path, category)\n",
    "            if not os.path.exists(category_path):\n",
    "                continue\n",
    "                \n",
    "            for image_name in os.listdir(category_path):\n",
    "                image_path = os.path.join(category_path, image_name)\n",
    "                try:\n",
    "                    with Image.open(image_path) as img:\n",
    "                        img.verify()\n",
    "                except:\n",
    "                    invalid_images.append(image_path)\n",
    "        \n",
    "        return invalid_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "227c4ead-3852-4f60-8e43-83b9b74cbd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"total_categories\": 6,\n",
      "  \"total_images\": 95,\n",
      "  \"categories\": {\n",
      "    \"biscuits\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"FRAC Cl\\u00e1sica.jpg\",\n",
      "          \"product_name\": \"FRAC Cl\\u00e1sica\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Gretel Chocolate.jpg\",\n",
      "          \"product_name\": \"Gretel Chocolate\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Ritz sabor a Queso.jpg\",\n",
      "          \"product_name\": \"Ritz sabor a Queso\",\n",
      "          \"brand\": \"Ritz\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Costa Choco Chips.jpg\",\n",
      "          \"product_name\": \"Costa Choco Chips\",\n",
      "          \"brand\": \"Costa\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM (4).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM.jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Emperador Nocturno.jpg\",\n",
      "          \"product_name\": \"Emperador Nocturno\",\n",
      "          \"brand\": \"Emperador\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Obsesi\\u00f3n.jpg\",\n",
      "          \"product_name\": \"Obsesi\\u00f3n\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Galletas de avena y pasas.jpg\",\n",
      "          \"product_name\": \"Galletas de avena y pasas\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"FRAC sabor Chocolate.jpg\",\n",
      "          \"product_name\": \"FRAC sabor Chocolate\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Costa Integrales.jpg\",\n",
      "          \"product_name\": \"Costa Integrales\",\n",
      "          \"brand\": \"Costa\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Gran Cereal Cl\\u00e1sica.jpg\",\n",
      "          \"product_name\": \"Gran Cereal Cl\\u00e1sica\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Gran Cereal Cacao.jpg\",\n",
      "          \"product_name\": \"Gran Cereal Cacao\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Galleta de Soda Min.jpg\",\n",
      "          \"product_name\": \"Galleta de Soda Min\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM (5).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Hony Bran.jpg\",\n",
      "          \"product_name\": \"Hony Bran\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.55 PM (4).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Galletitas Crackers Sin Sal.jpg\",\n",
      "          \"product_name\": \"Galletitas Crackers Sin Sal\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"FRAC sabor Vainilla.jpg\",\n",
      "          \"product_name\": \"FRAC sabor Vainilla\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"RITZ Sandwitch sabor a Queso.jpg\",\n",
      "          \"product_name\": \"RITZ Sandwitch sabor a Queso\",\n",
      "          \"brand\": \"Ritz\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Emperador Vainilla.jpg\",\n",
      "          \"product_name\": \"Emperador Vainilla\",\n",
      "          \"brand\": \"Emperador\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Casino sabor Chocolate.jpg\",\n",
      "          \"product_name\": \"Casino sabor Chocolate\",\n",
      "          \"brand\": \"Casino\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Casino sabor Fresa.jpg\",\n",
      "          \"product_name\": \"Casino sabor Fresa\",\n",
      "          \"brand\": \"Casino\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM (3).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Club Social Integral Tradicion.jpg\",\n",
      "          \"product_name\": \"Club Social Integral Tradicion\",\n",
      "          \"brand\": \"Club Social\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Club Social Original.jpg\",\n",
      "          \"product_name\": \"Club Social Original\",\n",
      "          \"brand\": \"Club Social\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Club Social Integral con Linaz.jpg\",\n",
      "          \"product_name\": \"Club Social Integral con Linaz\",\n",
      "          \"brand\": \"Club Social\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Oreo Chocolate.jpg\",\n",
      "          \"product_name\": \"Oreo Chocolate\",\n",
      "          \"brand\": \"Oreo\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Navide\\u00f1as.jpg\",\n",
      "          \"product_name\": \"Navide\\u00f1as\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nik Fresa.jpg\",\n",
      "          \"product_name\": \"Nik Fresa\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Noglut Maria.jpg\",\n",
      "          \"product_name\": \"Noglut Maria\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"CookiSanas Galletas Chocolate.jpg\",\n",
      "          \"product_name\": \"CookiSanas Galletas Chocolate\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (3).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM.jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Galletas RITZ.jpg\",\n",
      "          \"product_name\": \"Galletas RITZ\",\n",
      "          \"brand\": \"Ritz\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"CookiSanas Galletas Manzana.jpg\",\n",
      "          \"product_name\": \"CookiSanas Galletas Manzana\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Oreo Original.jpg\",\n",
      "          \"product_name\": \"Oreo Original\",\n",
      "          \"brand\": \"Oreo\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Casino sabor Vainilla.jpg\",\n",
      "          \"product_name\": \"Casino sabor Vainilla\",\n",
      "          \"brand\": \"Casino\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.56 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 42,\n",
      "      \"brands\": [\n",
      "        \"Club Social\",\n",
      "        \"Unknown Brand\",\n",
      "        \"Oreo\",\n",
      "        \"Costa\",\n",
      "        \"Ritz\",\n",
      "        \"Casino\",\n",
      "        \"Emperador\"\n",
      "      ]\n",
      "    },\n",
      "    \"canned\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.58 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.58 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (6).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.58 PM.jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (4).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Cut Green Beans.jpg\",\n",
      "          \"product_name\": \"Cut Green Beans\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.57 PM (5).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"atun en trozos desmenusado.jpg\",\n",
      "          \"product_name\": \"atun en trozos desmenusado\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 8,\n",
      "      \"brands\": [\n",
      "        \"Unknown Brand\"\n",
      "      ]\n",
      "    },\n",
      "    \"cereals\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"Arroz.jpg\",\n",
      "          \"product_name\": \"Arroz\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Trix conejitos con Marshmallow.jpg\",\n",
      "          \"product_name\": \"Trix conejitos con Marshmallow\",\n",
      "          \"brand\": \"Trix\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Harina de ma\\u00edz.jpg\",\n",
      "          \"product_name\": \"Harina de ma\\u00edz\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Gran Cereal Cl\\u00e1sica.jpg\",\n",
      "          \"product_name\": \"Gran Cereal Cl\\u00e1sica\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Gran Cereal Cacao.jpg\",\n",
      "          \"product_name\": \"Gran Cereal Cacao\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Macarrones con queso.jpg\",\n",
      "          \"product_name\": \"Macarrones con queso\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Check 3 Cereales.jpg\",\n",
      "          \"product_name\": \"Check 3 Cereales\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Espaguetis.jpg\",\n",
      "          \"product_name\": \"Espaguetis\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Aceite de ma\\u00edz.jpg\",\n",
      "          \"product_name\": \"Aceite de ma\\u00edz\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nesquik cereal.jpg\",\n",
      "          \"product_name\": \"Nesquik cereal\",\n",
      "          \"brand\": \"Nesquik\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Granola Crocante Almendras y P.jpg\",\n",
      "          \"product_name\": \"Granola Crocante Almendras y P\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nestum 5 cereales.jpg\",\n",
      "          \"product_name\": \"Nestum 5 cereales\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Corn Flakes Cereal.jpg\",\n",
      "          \"product_name\": \"Corn Flakes Cereal\",\n",
      "          \"brand\": \"Corn Flakes\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Avena Instant\\u00e1nea.jpg\",\n",
      "          \"product_name\": \"Avena Instant\\u00e1nea\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Check Cacao.jpg\",\n",
      "          \"product_name\": \"Check Cacao\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Granola con trocitos de Berrie.jpg\",\n",
      "          \"product_name\": \"Granola con trocitos de Berrie\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"hulo hoops.jpg\",\n",
      "          \"product_name\": \"hulo hoops\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Check sabor Yogurt.jpg\",\n",
      "          \"product_name\": \"Check sabor Yogurt\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Couscous Pre-Cooked.jpg\",\n",
      "          \"product_name\": \"Couscous Pre-Cooked\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Avena integral.jpg\",\n",
      "          \"product_name\": \"Avena integral\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Lasa\\u00f1a Tradicional.jpg\",\n",
      "          \"product_name\": \"Lasa\\u00f1a Tradicional\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Granola Crocante  Frutos Rojos.jpg\",\n",
      "          \"product_name\": \"Granola Crocante  Frutos Rojos\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Frutos del bosque.jpg\",\n",
      "          \"product_name\": \"Frutos del bosque\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Arroz Premium.jpg\",\n",
      "          \"product_name\": \"Arroz Premium\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nestum Trigo y Miel.jpg\",\n",
      "          \"product_name\": \"Nestum Trigo y Miel\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"almid\\u00f3n de ma\\u00edz.jpg\",\n",
      "          \"product_name\": \"almid\\u00f3n de ma\\u00edz\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 26,\n",
      "      \"brands\": [\n",
      "        \"Corn Flakes\",\n",
      "        \"Trix\",\n",
      "        \"Unknown Brand\",\n",
      "        \"Nesquik\"\n",
      "      ]\n",
      "    },\n",
      "    \"dried-foods\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"Sustagen Sabor Chocolate.jpg\",\n",
      "          \"product_name\": \"Sustagen Sabor Chocolate\",\n",
      "          \"brand\": \"Sustagen\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nesquik Chocolate.jpg\",\n",
      "          \"product_name\": \"Nesquik Chocolate\",\n",
      "          \"brand\": \"Nesquik\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Couscous Pre-Cooked.jpg\",\n",
      "          \"product_name\": \"Couscous Pre-Cooked\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 3,\n",
      "      \"brands\": [\n",
      "        \"Nesquik\",\n",
      "        \"Unknown Brand\",\n",
      "        \"Sustagen\"\n",
      "      ]\n",
      "    },\n",
      "    \"milk-powder\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.53 PM (3).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.53 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.55 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.55 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.55 PM (3).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.54 PM (2).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.54 PM.jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Nido Crecimiento Prebio 1.jpg\",\n",
      "          \"product_name\": \"Nido Crecimiento Prebio 1\",\n",
      "          \"brand\": \"Nido\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.55 PM.jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.53 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"WhatsApp Image 2025-01-28 at 7.33.54 PM (1).jpeg\",\n",
      "          \"product_name\": \"Unknown Product\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Leche Milex.jpg\",\n",
      "          \"product_name\": \"Leche Milex\",\n",
      "          \"brand\": \"Milex\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 12,\n",
      "      \"brands\": [\n",
      "        \"Nido\",\n",
      "        \"Milex\",\n",
      "        \"Unknown Brand\"\n",
      "      ]\n",
      "    },\n",
      "    \"water\": {\n",
      "      \"products\": [\n",
      "        {\n",
      "          \"filename\": \"Agua dasani.jpg\",\n",
      "          \"product_name\": \"Agua dasani\",\n",
      "          \"brand\": \"Dasani\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Club Soda.jpg\",\n",
      "          \"product_name\": \"Club Soda\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"lemon.jpg\",\n",
      "          \"product_name\": \"lemon\",\n",
      "          \"brand\": \"Unknown Brand\"\n",
      "        },\n",
      "        {\n",
      "          \"filename\": \"Planeta Azul.jpg\",\n",
      "          \"product_name\": \"Planeta Azul\",\n",
      "          \"brand\": \"Planeta Azul\"\n",
      "        }\n",
      "      ],\n",
      "      \"total_images\": 4,\n",
      "      \"brands\": [\n",
      "        \"Dasani\",\n",
      "        \"Unknown Brand\",\n",
      "        \"Planeta Azul\"\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    base_path = \"dataset/train/images\"\n",
    "    processor = DataProcessor(base_path)\n",
    "    \n",
    "    # Procesar dataset\n",
    "    processor.process_dataset()\n",
    "    \n",
    "    # Generar y guardar metadata\n",
    "    summary = processor.generate_dataset_summary()\n",
    "    processor.save_metadata(\"dataset_metadata.json\")\n",
    "    \n",
    "    # Validar imágenes\n",
    "    invalid_images = processor.validate_images()\n",
    "    if invalid_images:\n",
    "        print(\"Imágenes inválidas encontradas:\", invalid_images)\n",
    "    \n",
    "    # Imprimir resumen\n",
    "    print(json.dumps(summary, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30c4cb3e-b069-40d4-809d-31bd02a9be24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.26.0'                                ej3.jpeg\n",
      "'127_Carnation Evaporated Milk.jpg'      ej4.jpeg\n",
      " 197_leche.jpg                           forClaudeContex.ipynb\n",
      "'37_Planeta Azul.jpg'                    \u001b[0m\u001b[01;34mimagenes_productos\u001b[0m/\n",
      "'4_Club Social Integral Tradicion.jpg'   \u001b[01;34mmodels\u001b[0m/\n",
      "'7_Oreo Original.jpg'                    output.jpg\n",
      " catalog_metadata.json                   productos_rd.csv\n",
      " \u001b[01;34mdataset\u001b[0m/                                Untitled.ipynb\n",
      " \u001b[01;34mdataset2.0\u001b[0m/                             yolov8n.pt\n",
      " ej1.jpeg                                yolov8x.pt\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dd3aba2c-264e-40e6-88a4-0e4202eb06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcessor(\"dataset\")\n",
    "processor.process_dataset()\n",
    "summary = processor.generate_dataset_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "934f99e8-bd37-4b1c-9ba4-c3e31e7a7c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorías detectadas: ['entrenamiento', 'imagenes_productos', 'train', 'train (Copy)']\n",
      "Iniciando entrenamiento...\n",
      "Preparando datos desde: dataset\n",
      "Found 333 images belonging to 4 classes.\n",
      "Found 82 images belonging to 4 classes.\n",
      "Clases en el generador de entrenamiento: {'entrenamiento': 0, 'imagenes_productos': 1, 'train': 2, 'train (Copy)': 3}\n",
      "Número de clases: 4\n",
      "Tamaño del batch: 32\n",
      "Número de muestras de entrenamiento: 333\n",
      "Número de muestras de validación: 82\n",
      "Epoch 1/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 1s/step - accuracy: 0.3218 - loss: 1.4463 - val_accuracy: 0.2317 - val_loss: 1.4747\n",
      "Epoch 2/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 882ms/step - accuracy: 0.2784 - loss: 1.4203 - val_accuracy: 0.3537 - val_loss: 1.3767\n",
      "Epoch 3/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 880ms/step - accuracy: 0.3745 - loss: 1.3467 - val_accuracy: 0.3537 - val_loss: 1.4057\n",
      "Epoch 4/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 928ms/step - accuracy: 0.3126 - loss: 1.3759 - val_accuracy: 0.3537 - val_loss: 1.3690\n",
      "Epoch 5/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 954ms/step - accuracy: 0.3657 - loss: 1.3717 - val_accuracy: 0.2317 - val_loss: 1.3833\n",
      "Epoch 6/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.2410 - loss: 1.4132 - val_accuracy: 0.3537 - val_loss: 1.3625\n",
      "Epoch 7/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.3558 - loss: 1.3638 - val_accuracy: 0.3537 - val_loss: 1.3623\n",
      "Epoch 8/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 991ms/step - accuracy: 0.3386 - loss: 1.3836 - val_accuracy: 0.3537 - val_loss: 1.3670\n",
      "Epoch 9/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 974ms/step - accuracy: 0.3597 - loss: 1.3702 - val_accuracy: 0.3537 - val_loss: 1.3633\n",
      "Epoch 10/10\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 983ms/step - accuracy: 0.2864 - loss: 1.3718 - val_accuracy: 0.3537 - val_loss: 1.3736\n",
      "Iniciando fine-tuning...\n",
      "Iniciando entrenamiento...\n",
      "Preparando datos desde: dataset\n",
      "Found 333 images belonging to 4 classes.\n",
      "Found 82 images belonging to 4 classes.\n",
      "Clases en el generador de entrenamiento: {'entrenamiento': 0, 'imagenes_productos': 1, 'train': 2, 'train (Copy)': 3}\n",
      "Número de clases: 4\n",
      "Tamaño del batch: 32\n",
      "Número de muestras de entrenamiento: 333\n",
      "Número de muestras de validación: 82\n",
      "Epoch 1/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 1s/step - accuracy: 0.2427 - loss: 1.4538 - val_accuracy: 0.3537 - val_loss: 1.3626\n",
      "Epoch 2/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 1s/step - accuracy: 0.2209 - loss: 1.4268 - val_accuracy: 0.3537 - val_loss: 1.3627\n",
      "Epoch 3/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 1s/step - accuracy: 0.3648 - loss: 1.3664 - val_accuracy: 0.3537 - val_loss: 1.3627\n",
      "Epoch 4/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 1s/step - accuracy: 0.3452 - loss: 1.3722 - val_accuracy: 0.3537 - val_loss: 1.3626\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x764970775ac0>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = ProductClassifier(\"dataset\")\n",
    "classifier.train()\n",
    "classifier.fine_tune()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33a12f-0e6f-48e6-bcbf-f44453053414",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7db3a54-ca56-4e81-a26c-378413a35e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b187a3d7-ed27-4664-b0f1-6743a06366b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688b9d09-04c5-4660-ab36-b55be40b6cc3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ff06bf-70a5-47c2-9e32-152f928f01e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce454b2-51c2-487f-94cb-35c61ebf576d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f957c33b-2c9a-4888-9b4e-93847686b55a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9da298-72dd-411d-9b1d-8799f11b20ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ff1fb8-a21e-4128-9cc4-68e23a37a79c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77bf51-c11e-464c-b15c-d9d10bd272e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d954b-5769-41d9-8b45-9d73368b8937",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4328a267-46e9-406e-993e-2a895609ca42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4ecb5c-3136-4d47-9060-7fd56379fbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3ce4d-bc8f-4eb4-806b-064e2f0cd7e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f7dbdd-61c2-4abc-850a-38bf186db877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "25411ce1-b7e7-457a-baf8-80a44ba53ed1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicializando clasificador...\n",
      "Ollama está funcionando correctamente\n",
      "Usando dataset en: dataset/train/images\n",
      "\n",
      "Cargando dataset desde: /home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n",
      "Buscando categoría: biscuits en dataset/train/images/biscuits\n",
      "Encontrados 42 archivos en biscuits\n",
      "Buscando categoría: canned en dataset/train/images/canned\n",
      "Encontrados 8 archivos en canned\n",
      "Buscando categoría: cereals en dataset/train/images/cereals\n",
      "Encontrados 26 archivos en cereals\n",
      "Buscando categoría: dried-foods en dataset/train/images/dried-foods\n",
      "Encontrados 3 archivos en dried-foods\n",
      "Buscando categoría: milk-powder en dataset/train/images/milk-powder\n",
      "Encontrados 12 archivos en milk-powder\n",
      "Buscando categoría: water en dataset/train/images/water\n",
      "Encontrados 4 archivos en water\n",
      "\n",
      "Validando dataset en: /home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n",
      "Verificando categoría: biscuits\n",
      "Encontrados 42 archivos en biscuits\n",
      "Verificando categoría: canned\n",
      "Encontrados 8 archivos en canned\n",
      "Verificando categoría: cereals\n",
      "Encontrados 26 archivos en cereals\n",
      "Verificando categoría: dried-foods\n",
      "Encontrados 3 archivos en dried-foods\n",
      "Verificando categoría: milk-powder\n",
      "Encontrados 12 archivos en milk-powder\n",
      "Verificando categoría: water\n",
      "Encontrados 4 archivos en water\n",
      "\n",
      "Estadísticas del dataset:\n",
      "{\n",
      "  \"total_images\": 95,\n",
      "  \"categories\": {\n",
      "    \"biscuits\": {\n",
      "      \"count\": 42,\n",
      "      \"valid_images\": 42,\n",
      "      \"invalid_images\": 0\n",
      "    },\n",
      "    \"canned\": {\n",
      "      \"count\": 8,\n",
      "      \"valid_images\": 8,\n",
      "      \"invalid_images\": 0\n",
      "    },\n",
      "    \"cereals\": {\n",
      "      \"count\": 26,\n",
      "      \"valid_images\": 26,\n",
      "      \"invalid_images\": 0\n",
      "    },\n",
      "    \"dried-foods\": {\n",
      "      \"count\": 3,\n",
      "      \"valid_images\": 3,\n",
      "      \"invalid_images\": 0\n",
      "    },\n",
      "    \"milk-powder\": {\n",
      "      \"count\": 12,\n",
      "      \"valid_images\": 12,\n",
      "      \"invalid_images\": 0\n",
      "    },\n",
      "    \"water\": {\n",
      "      \"count\": 4,\n",
      "      \"valid_images\": 4,\n",
      "      \"invalid_images\": 0\n",
      "    }\n",
      "  },\n",
      "  \"invalid_images\": []\n",
      "}\n",
      "\n",
      "Clasificando imagen: 197_leche.jpg\n",
      "Ollama está funcionando correctamente\n",
      "Codificando imagen: 197_leche.jpg\n",
      "\n",
      "Prompt generado:\n",
      "Analiza esta imagen '197_leche.jpg' y clasifícala en una de las siguientes categorías: biscuits, canned, cereals, dried-foods, milk-powder, water. Identifica también la marca del producto. Las marcas comunes para cada categoría son:\n",
      "- biscuits: Casino, Club Social, Oreo, Ritz, Costa, Emperador, FRAC, Guarina\n",
      "- canned: La Famosa, Goya, Del Monte\n",
      "- cereals: Nesquik, Corn Flakes, Trix, Gran Cereal\n",
      "- dried-foods: Nesquik, Sustagen\n",
      "- milk-powder: Milex, Nido, Carnation\n",
      "- water: Planeta Azul, Dasani, Cascada\n",
      "\n",
      "Ejemplos de productos conocidos:\n",
      "- biscuits: FRAC Clásica, Gretel Chocolate, Ritz sabor a Queso\n",
      "- canned: Cut Green Beans, atun en trozos desmenusado\n",
      "- cereals: Arroz, Trix conejitos con Marshmallow, Harina de maíz\n",
      "- dried-foods: Sustagen Sabor Chocolate, Nesquik Chocolate, Couscous Pre-Cooked\n",
      "- milk-powder: Nido Crecimiento Prebio 1, Leche Milex\n",
      "- water: Agua dasani, Club Soda, lemon\n",
      "\n",
      "Por favor, responde SOLO con el formato 'categoría (marca)'\n",
      "\n",
      "Enviando solicitud a Ollama...\n",
      "Código de respuesta: 200\n",
      "Predicción recibida:  desculpe, la imagen que proporcionaste no parece mostrar un producto de alimentación, por lo tanto no puedo clasificarlo en una de las categorías que mencionas. si tienes algún otro producto o algo relacionado con alimentación, pueda enviar la imagen y yo le ayudaré a identificar la categoría correspondiente. \n",
      "\n",
      "Resultado de clasificación:\n",
      "Categoría: desconocido\n",
      "Marca: marca desconocida\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "class EnhancedVisionClassifier:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "        self.MODEL_NAME = \"llava\"  #llama3.2-vision\n",
    "        self.categories = {\n",
    "            'biscuits': ['Casino', 'Club Social', 'Oreo', 'Ritz', 'Costa', 'Emperador', 'FRAC', 'Guarina'],\n",
    "            'canned': ['La Famosa', 'Goya', 'Del Monte'],\n",
    "            'cereals': ['Nesquik', 'Corn Flakes', 'Trix', 'Gran Cereal'],\n",
    "            'dried-foods': ['Nesquik', 'Sustagen'],\n",
    "            'milk-powder': ['Milex', 'Nido', 'Carnation'],\n",
    "            'water': ['Planeta Azul', 'Dasani', 'Cascada']\n",
    "        }\n",
    "        self.dataset_knowledge = {}\n",
    "        \n",
    "    def verify_ollama(self):\n",
    "        \"\"\"Verifica que Ollama esté funcionando\"\"\"\n",
    "        try:\n",
    "            response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "            if response.status_code == 200:\n",
    "                print(\"Ollama está funcionando correctamente\")\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"Ollama respondió con código de estado: {response.status_code}\")\n",
    "                return False\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"No se pudo conectar a Ollama. ¿Está corriendo el servicio?\")\n",
    "            return False\n",
    "        \n",
    "    def load_dataset_knowledge(self, dataset_path: str):\n",
    "        \"\"\"Carga conocimiento del dataset para mejorar las predicciones\"\"\"\n",
    "        print(f\"\\nCargando dataset desde: {os.path.abspath(dataset_path)}\")\n",
    "        \n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: El directorio {dataset_path} no existe\")\n",
    "            return\n",
    "            \n",
    "        for category in self.categories.keys():\n",
    "            category_path = os.path.join(dataset_path, category)\n",
    "            print(f\"Buscando categoría: {category} en {category_path}\")\n",
    "            \n",
    "            if os.path.exists(category_path):\n",
    "                self.dataset_knowledge[category] = []\n",
    "                files = os.listdir(category_path)\n",
    "                print(f\"Encontrados {len(files)} archivos en {category}\")\n",
    "                \n",
    "                for img_name in files:\n",
    "                    if \"WhatsApp Image\" not in img_name:\n",
    "                        product_name = os.path.splitext(img_name)[0]\n",
    "                        self.dataset_knowledge[category].append(product_name)\n",
    "            else:\n",
    "                print(f\"No se encontró el directorio para la categoría: {category}\")\n",
    "\n",
    "    def generate_enhanced_prompt(self, image_path: str) -> str:\n",
    "        \"\"\"Genera un prompt mejorado basado en el conocimiento del dataset\"\"\"\n",
    "        base_prompt = (\n",
    "            f\"Analiza esta imagen '{os.path.basename(image_path)}' y clasifícala en una de las siguientes categorías: \"\n",
    "            f\"{', '.join(self.categories.keys())}. \"\n",
    "            \"Identifica también la marca del producto. \"\n",
    "            \"Las marcas comunes para cada categoría son:\\n\"\n",
    "        )\n",
    "        \n",
    "        for cat, brands in self.categories.items():\n",
    "            base_prompt += f\"- {cat}: {', '.join(brands)}\\n\"\n",
    "        \n",
    "        if self.dataset_knowledge:\n",
    "            base_prompt += \"\\nEjemplos de productos conocidos:\\n\"\n",
    "            for cat, products in self.dataset_knowledge.items():\n",
    "                if products:\n",
    "                    base_prompt += f\"- {cat}: {', '.join(products[:3])}\\n\"\n",
    "        \n",
    "        base_prompt += \"\\nPor favor, responde SOLO con el formato 'categoría (marca)'\"\n",
    "        print(f\"\\nPrompt generado:\\n{base_prompt}\\n\")\n",
    "        return base_prompt\n",
    "\n",
    "    def encode_image(self, image_path: str) -> str:\n",
    "        \"\"\"Codifica la imagen a base64\"\"\"\n",
    "        if not os.path.exists(image_path):\n",
    "            print(f\"Error: La imagen {image_path} no existe\")\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            print(f\"Codificando imagen: {image_path}\")\n",
    "            with Image.open(image_path) as img:\n",
    "                if img.mode != 'RGB':\n",
    "                    img = img.convert('RGB')\n",
    "                buffer = io.BytesIO()\n",
    "                img.save(buffer, format='JPEG')\n",
    "                return base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
    "        except Exception as e:\n",
    "            print(f\"Error al codificar la imagen: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "    def classify_image(self, image_path: str) -> Tuple[str, str]:\n",
    "        \"\"\"Clasifica una imagen usando el modelo llama3.2-vision mejorado\"\"\"\n",
    "        # Verificar Ollama\n",
    "        if not self.verify_ollama():\n",
    "            return \"error\", \"error (Ollama no está disponible)\"\n",
    "            \n",
    "        # Verificar y codificar imagen\n",
    "        base64_image = self.encode_image(image_path)\n",
    "        if not base64_image:\n",
    "            return \"error\", \"error (imagen no válida)\"\n",
    "\n",
    "        prompt = self.generate_enhanced_prompt(image_path)\n",
    "        \n",
    "        payload = {\n",
    "            \"model\": self.MODEL_NAME,\n",
    "            \"prompt\": prompt,\n",
    "            \"stream\": False,\n",
    "            \"images\": [base64_image]\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            print(\"Enviando solicitud a Ollama...\")\n",
    "            response = requests.post(self.BASE_URL, json=payload)\n",
    "            print(f\"Código de respuesta: {response.status_code}\")\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                result = response.json()\n",
    "                prediction = result.get('response', '').lower()\n",
    "                print(f\"Predicción recibida: {prediction}\")\n",
    "                \n",
    "                # Extraer categoría y marca\n",
    "                for category in self.categories.keys():\n",
    "                    if category in prediction:\n",
    "                        # Buscar marca\n",
    "                        for brand in self.categories[category]:\n",
    "                            if brand.lower() in prediction:\n",
    "                                return category, brand\n",
    "                        return category, \"marca desconocida\"\n",
    "                \n",
    "                return \"desconocido\", \"marca desconocida\"\n",
    "            else:\n",
    "                print(f\"Error en la respuesta: {response.text}\")\n",
    "                return \"error\", f\"error (código {response.status_code})\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error en la clasificación: {str(e)}\")\n",
    "            return \"error\", f\"error ({str(e)})\"\n",
    "\n",
    "    def validate_dataset(self, dataset_path: str) -> Dict:\n",
    "        \"\"\"Valida el dataset y genera estadísticas\"\"\"\n",
    "        stats = {\n",
    "            'total_images': 0,\n",
    "            'categories': {},\n",
    "            'invalid_images': []\n",
    "        }\n",
    "        \n",
    "        print(f\"\\nValidando dataset en: {os.path.abspath(dataset_path)}\")\n",
    "        \n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: El directorio del dataset no existe: {dataset_path}\")\n",
    "            return stats\n",
    "        \n",
    "        for category in self.categories.keys():\n",
    "            category_path = os.path.join(dataset_path, category)\n",
    "            print(f\"Verificando categoría: {category}\")\n",
    "            \n",
    "            if os.path.exists(category_path):\n",
    "                stats['categories'][category] = {\n",
    "                    'count': 0,\n",
    "                    'valid_images': 0,\n",
    "                    'invalid_images': 0\n",
    "                }\n",
    "                \n",
    "                files = os.listdir(category_path)\n",
    "                print(f\"Encontrados {len(files)} archivos en {category}\")\n",
    "                \n",
    "                for img_name in files:\n",
    "                    img_path = os.path.join(category_path, img_name)\n",
    "                    stats['categories'][category]['count'] += 1\n",
    "                    stats['total_images'] += 1\n",
    "                    \n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            img.verify()\n",
    "                            stats['categories'][category]['valid_images'] += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"Imagen inválida encontrada: {img_path}\")\n",
    "                        print(f\"Error: {str(e)}\")\n",
    "                        stats['categories'][category]['invalid_images'] += 1\n",
    "                        stats['invalid_images'].append(img_path)\n",
    "            else:\n",
    "                print(f\"No se encontró el directorio para la categoría: {category}\")\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    # Inicializar el clasificador\n",
    "    print(\"Inicializando clasificador...\")\n",
    "    classifier = EnhancedVisionClassifier()\n",
    "    \n",
    "    # Verificar Ollama\n",
    "    if not classifier.verify_ollama():\n",
    "        print(\"Error: No se puede continuar sin Ollama funcionando\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Construir la ruta completa al dataset\n",
    "    base_path = \"dataset\"\n",
    "    dataset_path = os.path.join(base_path, \"train\", \"images\")\n",
    "    \n",
    "    # Verificar que el directorio existe\n",
    "    if not os.path.exists(dataset_path):\n",
    "        print(f\"Error: No se encontró el directorio del dataset en: {dataset_path}\")\n",
    "        # Intentar con ruta absoluta como fallback\n",
    "        dataset_path = \"/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\"\n",
    "        if not os.path.exists(dataset_path):\n",
    "            print(f\"Error: Tampoco se encontró el dataset en la ruta absoluta: {dataset_path}\")\n",
    "            exit(1)\n",
    "    \n",
    "    print(f\"Usando dataset en: {dataset_path}\")\n",
    "    \n",
    "    # Cargar conocimiento del dataset\n",
    "    classifier.load_dataset_knowledge(dataset_path)\n",
    "    \n",
    "    # Validar dataset\n",
    "    stats = classifier.validate_dataset(dataset_path)\n",
    "    print(\"\\nEstadísticas del dataset:\")\n",
    "    print(json.dumps(stats, indent=2))\n",
    "    \n",
    "    # Ejemplo de clasificación\n",
    "   # test_image = \"37_Planeta Azul.jpg\"  # Asumiendo que la imagen está en el directorio actual\n",
    "    test_image = \"197_leche.jpg\"\n",
    "    print(f\"\\nClasificando imagen: {test_image}\")\n",
    "    category, brand = classifier.classify_image(test_image)\n",
    "    print(f\"\\nResultado de clasificación:\")\n",
    "    print(f\"Categoría: {category}\")\n",
    "    print(f\"Marca: {brand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bf0d93e3-2ec2-43cf-ac98-39ff79c49b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbiscuits\u001b[0m/  \u001b[01;34mcanned\u001b[0m/  \u001b[01;34mcereals\u001b[0m/  \u001b[01;34mdried-foods\u001b[0m/  \u001b[01;34mmilk-powder\u001b[0m/  \u001b[01;34mwater\u001b[0m/\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6b170ad1-f869-427d-a8bb-0edfbac60dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset\n"
     ]
    }
   ],
   "source": [
    "cd dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "82dce2c5-3c6c-4989-a539-3dad4b3a1c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'=0.26.0'                                ej3.jpeg\n",
      "'127_Carnation Evaporated Milk.jpg'      ej4.jpeg\n",
      " 197_leche.jpg                           forClaudeContex.ipynb\n",
      "'37_Planeta Azul.jpg'                    \u001b[0m\u001b[01;34mimagenes_productos\u001b[0m/\n",
      "'4_Club Social Integral Tradicion.jpg'   \u001b[01;34mmodels\u001b[0m/\n",
      "'7_Oreo Original.jpg'                    output.jpg\n",
      " catalog_metadata.json                   productos_rd.csv\n",
      " \u001b[01;34mdataset\u001b[0m/                                Untitled.ipynb\n",
      " \u001b[01;34mdataset2.0\u001b[0m/                             yolov8n.pt\n",
      " dataset_metadata.json                   yolov8x.pt\n",
      " ej1.jpeg\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6079c559-95ee-4d10-9a8d-888e9a3a84e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train\n"
     ]
    }
   ],
   "source": [
    "cd train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f5230095-f1e6-4aa8-a835-4a217ec7abc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook/dataset/train/images\n"
     ]
    }
   ],
   "source": [
    "cd images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d6408efa-0312-4f5f-98eb-8e884566812f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/friasluna/Tesis_Proyect/proyecto_tesis/modelo/jupyer_notebook\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "4395da7b-6d07-4091-8203-948be82fb879",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple\n",
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class EnhancedVisionClassifier:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "        self.MODEL_NAME = \"llava\"\n",
    "        self.categories = {\n",
    "            'biscuits': {\n",
    "                'brands': ['Casino', 'Club Social', 'Oreo', 'Ritz', 'Costa', 'Emperador', 'FRAC', 'Guarina', 'Max'],\n",
    "                'keywords': ['galleta', 'cookie', 'cracker', 'wafer', 'biscuit', 'snack']\n",
    "            },\n",
    "            'canned': {\n",
    "                'brands': ['La Famosa', 'Goya', 'Del Monte'],\n",
    "                'keywords': ['enlatado', 'conserva', 'lata', 'atún', 'sardina']\n",
    "            },\n",
    "            'cereals': {\n",
    "                'brands': ['Nesquik', 'Corn Flakes', 'Trix', 'Gran Cereal'],\n",
    "                'keywords': ['cereal', 'avena', 'corn', 'granola', 'harina']\n",
    "            },\n",
    "            'dried-foods': {\n",
    "                'brands': ['Nesquik', 'Sustagen'],\n",
    "                'keywords': ['polvo', 'deshidratado', 'instantáneo']\n",
    "            },\n",
    "            'milk-powder': {\n",
    "                'brands': ['Milex', 'Nido', 'Carnation', 'Leche Entera'],\n",
    "                'keywords': ['leche', 'milk', 'lácteo', 'fórmula']\n",
    "            },\n",
    "            'water': {\n",
    "                'brands': ['Planeta Azul', 'Dasani', 'Cascada'],\n",
    "                'keywords': ['agua', 'water', 'bebida', 'botella']\n",
    "            }\n",
    "        }\n",
    "        self.dataset_knowledge = {}\n",
    "        self.product_features = {}\n",
    "        \n",
    "    def load_and_process_dataset(self, dataset_path: str):\n",
    "        \"\"\"Carga y procesa el dataset, extrayendo características visuales de las imágenes\"\"\"\n",
    "        print(f\"\\nProcesando dataset en: {dataset_path}\")\n",
    "        \n",
    "        for category in self.categories.keys():\n",
    "            category_path = Path(dataset_path) / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "                \n",
    "            self.product_features[category] = []\n",
    "            for img_file in category_path.glob('*.[jJ][pP][gG]'):\n",
    "                if 'WhatsApp' not in img_file.name:\n",
    "                    try:\n",
    "                        # Extraer características visuales\n",
    "                        img = cv2.imread(str(img_file))\n",
    "                        if img is None:\n",
    "                            continue\n",
    "                            \n",
    "                        # Convertir a RGB y redimensionar\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, (224, 224))\n",
    "                        \n",
    "                        # Extraer características básicas\n",
    "                        features = {\n",
    "                            'name': img_file.stem,\n",
    "                            'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                            'dominant_colors': self._get_dominant_colors(img),\n",
    "                            'edges': self._get_edge_density(img)\n",
    "                        }\n",
    "                        \n",
    "                        self.product_features[category].append(features)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error procesando {img_file}: {e}\")\n",
    "                        \n",
    "        print(\"Dataset procesado y características extraídas\")\n",
    "\n",
    "    def _get_dominant_colors(self, img, n_colors=5):\n",
    "        \"\"\"Extrae los colores dominantes de una imagen\"\"\"\n",
    "        pixels = img.reshape(-1, 3)\n",
    "        pixels = np.float32(pixels)\n",
    "        \n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
    "        flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "        \n",
    "        _, labels, centers = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        return centers[np.argsort(-counts)]\n",
    "        \n",
    "    def _get_edge_density(self, img):\n",
    "        \"\"\"Calcula la densidad de bordes en la imagen\"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        return np.mean(edges > 0)\n",
    "\n",
    "    def generate_enhanced_prompt(self, image_path: str, image_features: Dict) -> str:\n",
    "        \"\"\"Genera un prompt mejorado basado en características de la imagen\"\"\"\n",
    "        base_prompt = (\n",
    "            f\"Analiza esta imagen y clasifícala. La imagen muestra un producto que podría ser:\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        # Agregar información específica de cada categoría\n",
    "        for category, info in self.categories.items():\n",
    "            base_prompt += f\"- {category.upper()}: {', '.join(info['brands'])}\\n\"\n",
    "            base_prompt += f\"  Características típicas: {', '.join(info['keywords'])}\\n\"\n",
    "        \n",
    "        # Agregar información sobre características visuales detectadas\n",
    "        base_prompt += \"\\nCaracterísticas detectadas en la imagen:\\n\"\n",
    "        base_prompt += f\"- Densidad de bordes: {'alta' if image_features['edges'] > 0.1 else 'baja'}\\n\"\n",
    "        base_prompt += f\"- Colores dominantes: {len(image_features['dominant_colors'])} colores principales\\n\"\n",
    "        \n",
    "        base_prompt += \"\\nResponde SOLO con el formato: 'categoría (marca)'\"\n",
    "        return base_prompt\n",
    "\n",
    "    def classify_image(self, image_path: str) -> Tuple[str, str]:\n",
    "        \"\"\"Clasifica una imagen usando el modelo mejorado\"\"\"\n",
    "        try:\n",
    "            # Cargar y procesar la imagen\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return \"error\", \"error (imagen no válida)\"\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            \n",
    "            # Extraer características\n",
    "            image_features = {\n",
    "                'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                'dominant_colors': self._get_dominant_colors(img),\n",
    "                'edges': self._get_edge_density(img)\n",
    "            }\n",
    "            \n",
    "            # Generar prompt mejorado\n",
    "            prompt = self.generate_enhanced_prompt(image_path, image_features)\n",
    "            \n",
    "            # Codificar imagen\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            # Preparar payload\n",
    "            payload = {\n",
    "                \"model\": self.MODEL_NAME,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"images\": [base64_image]\n",
    "            }\n",
    "            \n",
    "            # Realizar petición\n",
    "            response = requests.post(self.BASE_URL, json=payload)\n",
    "            if response.status_code != 200:\n",
    "                return \"error\", f\"error (código {response.status_code})\"\n",
    "                \n",
    "            result = response.json()\n",
    "            prediction = result.get('response', '').lower()\n",
    "            \n",
    "            # Procesar predicción\n",
    "            best_category = None\n",
    "            best_brand = None\n",
    "            max_score = 0\n",
    "            \n",
    "            for category, info in self.categories.items():\n",
    "                # Verificar keywords\n",
    "                keyword_score = sum(1 for keyword in info['keywords'] if keyword in prediction)\n",
    "                \n",
    "                # Verificar marcas\n",
    "                brand_score = 0\n",
    "                detected_brand = None\n",
    "                for brand in info['brands']:\n",
    "                    if brand.lower() in prediction:\n",
    "                        brand_score = 1\n",
    "                        detected_brand = brand\n",
    "                        break\n",
    "                \n",
    "                total_score = keyword_score + (brand_score * 2)\n",
    "                if total_score > max_score:\n",
    "                    max_score = total_score\n",
    "                    best_category = category\n",
    "                    best_brand = detected_brand\n",
    "            \n",
    "            if best_category:\n",
    "                return best_category, best_brand if best_brand else \"marca desconocida\"\n",
    "            \n",
    "            return \"desconocido\", \"marca desconocida\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en clasificación: {e}\")\n",
    "            return \"error\", f\"error ({str(e)})\"\n",
    "\n",
    "    def batch_classify(self, image_paths: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Clasifica un lote de imágenes\"\"\"\n",
    "        results = []\n",
    "        for path in image_paths:\n",
    "            category, brand = self.classify_image(path)\n",
    "            results.append((path, category, brand))\n",
    "        return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f51b1ace-08ba-4b13-b638-d81d463877be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando dataset en: dataset/train/images\n",
      "Dataset procesado y características extraídas\n",
      "\n",
      "Resultado de clasificación:\n",
      "Imagen: ej4.jpeg\n",
      "Categoría: milk-powder\n",
      "Marca: Nido\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = EnhancedVisionClassifier()\n",
    "    \n",
    "    # Configurar y cargar dataset\n",
    "    dataset_path = \"dataset/train/images\"\n",
    "    classifier.load_and_process_dataset(dataset_path)\n",
    "    \n",
    "    # Ejemplo de clasificación\n",
    "    test_image = \"ej4.jpeg\"\n",
    "    category, brand = classifier.classify_image(test_image)\n",
    "    print(f\"\\nResultado de clasificación:\")\n",
    "    print(f\"Imagen: {test_image}\")\n",
    "    print(f\"Categoría: {category}\")\n",
    "    print(f\"Marca: {brand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94adde1d-3129-4b13-be0a-6f150c143827",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1bcf3-a343-49c1-99f5-333e919391df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449fb8ed-629c-42ca-b879-c2fdc182364f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7507efcd-16e6-4357-96ed-70f35890810d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4e977a-7a34-4361-86e1-03c59d5850f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570cfa90-82dc-4c19-8751-9819be5dfc03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707694f-eb14-41e6-907d-f79e06f3697d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "18c031bb-f286-4d81-9bb8-a36ee31848e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando dataset en: dataset/train/images\n",
      "Dataset procesado y características extraídas\n",
      "\n",
      "Resultado de clasificación:\n",
      "Imagen: ej4.jpeg\n",
      "Categoría: {'category': 'milk-powder', 'brand': 'leche entera', 'quantity': 4}\n",
      "Marca: {'category': 'cereals', 'brand': 'corn flakes', 'quantity': 1}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class EnhancedVisionClassifier:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "        self.MODEL_NAME = \"llava:13\"\n",
    "        self.categories = {\n",
    "            'biscuits': {\n",
    "                'brands': ['Casino', 'Club Social', 'Oreo', 'Ritz', 'Costa', 'Emperador', 'FRAC', 'Guarina'],\n",
    "                'keywords': ['galleta', 'cookie', 'cracker', 'wafer', 'biscuit', 'snack']\n",
    "            },\n",
    "            'canned': {\n",
    "                'brands': ['La Famosa', 'Goya', 'Del Monte'],\n",
    "                'keywords': ['enlatado', 'conserva', 'lata', 'atún', 'sardina']\n",
    "            },\n",
    "            'cereals': {\n",
    "                'brands': ['Nesquik', 'Corn Flakes', 'Trix', 'Gran Cereal'],\n",
    "                'keywords': ['cereal', 'avena', 'corn', 'granola', 'harina']\n",
    "            },\n",
    "            'dried-foods': {\n",
    "                'brands': ['Nesquik', 'Sustagen'],\n",
    "                'keywords': ['polvo', 'deshidratado', 'instantáneo']\n",
    "            },\n",
    "            'milk-powder': {\n",
    "                'brands': ['Milex', 'Nido', 'Carnation'],\n",
    "                'keywords': ['leche', 'milk', 'lácteo', 'fórmula']\n",
    "            },\n",
    "            'water': {\n",
    "                'brands': ['Planeta Azul', 'Dasani', 'Cascada'],\n",
    "                'keywords': ['agua', 'water', 'bebida', 'botella']\n",
    "            }\n",
    "        }\n",
    "        self.dataset_knowledge = {}\n",
    "        self.product_features = {}\n",
    "        \n",
    "    def load_and_process_dataset(self, dataset_path: str):\n",
    "        \"\"\"Carga y procesa el dataset, extrayendo características visuales de las imágenes\"\"\"\n",
    "        print(f\"\\nProcesando dataset en: {dataset_path}\")\n",
    "        \n",
    "        for category in self.categories.keys():\n",
    "            category_path = Path(dataset_path) / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "                \n",
    "            self.product_features[category] = []\n",
    "            for img_file in category_path.glob('*.[jJ][pP][gG]'):\n",
    "                if 'WhatsApp' not in img_file.name:\n",
    "                    try:\n",
    "                        # Extraer características visuales\n",
    "                        img = cv2.imread(str(img_file))\n",
    "                        if img is None:\n",
    "                            continue\n",
    "                            \n",
    "                        # Convertir a RGB y redimensionar\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, (224, 224))\n",
    "                        \n",
    "                        # Extraer características básicas\n",
    "                        features = {\n",
    "                            'name': img_file.stem,\n",
    "                            'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                            'dominant_colors': self._get_dominant_colors(img),\n",
    "                            'edges': self._get_edge_density(img)\n",
    "                        }\n",
    "                        \n",
    "                        self.product_features[category].append(features)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error procesando {img_file}: {e}\")\n",
    "                        \n",
    "        print(\"Dataset procesado y características extraídas\")\n",
    "\n",
    "    def _get_dominant_colors(self, img, n_colors=5):\n",
    "        \"\"\"Extrae los colores dominantes de una imagen\"\"\"\n",
    "        pixels = img.reshape(-1, 3)\n",
    "        pixels = np.float32(pixels)\n",
    "        \n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
    "        flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "        \n",
    "        _, labels, centers = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        return centers[np.argsort(-counts)]\n",
    "        \n",
    "    def _get_edge_density(self, img):\n",
    "        \"\"\"Calcula la densidad de bordes en la imagen\"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        return np.mean(edges > 0)\n",
    "\n",
    "    def generate_enhanced_prompt(self, image_path: str, image_features: Dict) -> str:\n",
    "        \"\"\"Genera un prompt mejorado basado en características de la imagen\"\"\"\n",
    "        base_prompt = (\n",
    "            f\"Analiza detalladamente esta imagen y enumera TODOS los productos visibles. \"\n",
    "            f\"IMPORTANTE: Si hay múltiples unidades del mismo producto, cuéntalas. \"\n",
    "            f\"Si hay productos diferentes en la misma imagen, enuméralos por separado.\\n\\n\"\n",
    "            f\"Para cada producto que veas, necesito:\\n\"\n",
    "            f\"1. Categoría exacta (elige de la lista proporcionada)\\n\"\n",
    "            f\"2. Marca específica\\n\"\n",
    "            f\"3. Cantidad precisa de unidades\\n\\n\"\n",
    "            f\"Para la categoría milk-powder, busca específicamente:\\n\"\n",
    "            f\"- Cajas o envases de leche\\n\"\n",
    "            f\"- Leche en polvo o líquida\\n\"\n",
    "            f\"- Marcas como Milex, Nido, Carnation o Leche Entera\\n\\n\"\n",
    "            f\"Los productos deben clasificarse en una de estas categorías:\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        # Agregar información específica de cada categoría\n",
    "        for category, info in self.categories.items():\n",
    "            base_prompt += f\"- {category.upper()}: {', '.join(info['brands'])}\\n\"\n",
    "            base_prompt += f\"  Características típicas: {', '.join(info['keywords'])}\\n\"\n",
    "        \n",
    "        # Agregar información sobre características visuales detectadas\n",
    "        base_prompt += \"\\nCaracterísticas detectadas en la imagen:\\n\"\n",
    "        base_prompt += f\"- Densidad de bordes: {'alta' if image_features['edges'] > 0.1 else 'baja'}\\n\"\n",
    "        base_prompt += f\"- Colores dominantes: {len(image_features['dominant_colors'])} colores principales\\n\"\n",
    "        \n",
    "        base_prompt += \"\\nResponde en formato JSON como este ejemplo:\\n\"\n",
    "        base_prompt += \"\"\"[\n",
    "            {\"category\": \"milk-powder\", \"brand\": \"Leche Entera\", \"quantity\": 4},\n",
    "            {\"category\": \"cereals\", \"brand\": \"Corn Flakes\", \"quantity\": 1}\n",
    "        ]\"\"\"\n",
    "        return base_prompt\n",
    "\n",
    "    def classify_image(self, image_path: str) -> List[Dict[str, Union[str, int]]]:\n",
    "        \"\"\"Clasifica múltiples productos en una imagen y cuenta cantidades\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Lista de productos detectados con su categoría, marca y cantidad\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Cargar y procesar la imagen\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return \"error\", \"error (imagen no válida)\"\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            \n",
    "            # Extraer características\n",
    "            image_features = {\n",
    "                'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                'dominant_colors': self._get_dominant_colors(img),\n",
    "                'edges': self._get_edge_density(img)\n",
    "            }\n",
    "            \n",
    "            # Generar prompt mejorado\n",
    "            prompt = self.generate_enhanced_prompt(image_path, image_features)\n",
    "            \n",
    "            # Codificar imagen\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            # Preparar payload\n",
    "            payload = {\n",
    "                \"model\": self.MODEL_NAME,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"images\": [base64_image]\n",
    "            }\n",
    "            \n",
    "            # Realizar petición\n",
    "            response = requests.post(self.BASE_URL, json=payload)\n",
    "            if response.status_code != 200:\n",
    "                return \"error\", f\"error (código {response.status_code})\"\n",
    "                \n",
    "            result = response.json()\n",
    "            prediction = result.get('response', '').lower()\n",
    "            \n",
    "            # Procesar predicción\n",
    "            products = []\n",
    "            \n",
    "            # Primero intentar encontrar un formato JSON en la respuesta\n",
    "            json_matches = re.findall(r'\\[.*\\]', prediction.replace('\\n', ' '))\n",
    "            if json_matches:\n",
    "                try:\n",
    "                    products = json.loads(json_matches[0])\n",
    "                    if isinstance(products, list):\n",
    "                        return products\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "                    \n",
    "            # Si no se encontró JSON válido, procesar el texto\n",
    "            print(\"Procesando respuesta en texto plano:\", prediction)\n",
    "            # Procesamiento mejorado del texto\n",
    "            found_products = {}\n",
    "            \n",
    "            # Patrones de cantidad más específicos\n",
    "            quantity_patterns = [\n",
    "                r'(\\d+)\\s*(?:unidades?|cajas?|paquetes?|botellas?|envases?)',\n",
    "                r'hay\\s*(\\d+)',\n",
    "                r'se ven\\s*(\\d+)',\n",
    "                r'(\\d+)\\s*productos?',\n",
    "                r'cantidad[:\\s]+(\\d+)',\n",
    "                r'(\\d+)\\s*(?:milk|leche|agua|galletas?|cereales?)'\n",
    "            ]\n",
    "            \n",
    "            # Procesar por categoría\n",
    "            for category, info in self.categories.items():\n",
    "                category_keywords = set(info['keywords'])\n",
    "                category_brands = set(brand.lower() for brand in info['brands'])\n",
    "                \n",
    "                # Buscar coincidencias de categoría\n",
    "                if any(keyword in prediction.lower() for keyword in category_keywords):\n",
    "                    # Buscar marcas específicas\n",
    "                    for brand in info['brands']:\n",
    "                        brand_lower = brand.lower()\n",
    "                        if brand_lower in prediction.lower():\n",
    "                            product_key = f\"{category}-{brand}\"\n",
    "                            if product_key not in found_products:\n",
    "                                # Buscar cantidad usando todos los patrones\n",
    "                                quantities = []\n",
    "                                for pattern in quantity_patterns:\n",
    "                                    matches = re.findall(pattern, prediction.lower())\n",
    "                                    quantities.extend(matches)\n",
    "                                \n",
    "                                # Si encontramos cantidades, usar la más grande\n",
    "                                quantity = 1\n",
    "                                if quantities:\n",
    "                                    quantity = max(int(q) for q in quantities if q.isdigit())\n",
    "                                \n",
    "                                products.append({\n",
    "                                    \"category\": category,\n",
    "                                    \"brand\": brand,\n",
    "                                    \"quantity\": quantity\n",
    "                                })\n",
    "                                found_products[product_key] = True\n",
    "                                \n",
    "                                print(f\"Encontrado: {category} - {brand} - {quantity} unidades\")\n",
    "            \n",
    "            if not products:\n",
    "                products.append({\n",
    "                    \"category\": \"desconocido\",\n",
    "                    \"brand\": \"marca desconocida\",\n",
    "                    \"quantity\": 1\n",
    "                })\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en clasificación: {e}\")\n",
    "            return \"error\", f\"error ({str(e)})\"\n",
    "\n",
    "    def batch_classify(self, image_paths: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Clasifica un lote de imágenes\"\"\"\n",
    "        results = []\n",
    "        for path in image_paths:\n",
    "            category, brand = self.classify_image(path)\n",
    "            results.append((path, category, brand))\n",
    "        return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = EnhancedVisionClassifier()\n",
    "    \n",
    "    # Configurar y cargar dataset\n",
    "    dataset_path = \"dataset/train/images\"\n",
    "    classifier.load_and_process_dataset(dataset_path)\n",
    "    \n",
    "    # Ejemplo de clasificación\n",
    "    test_image = \"ej4.jpeg\"\n",
    "    category, brand = classifier.classify_image(test_image)\n",
    "    print(f\"\\nResultado de clasificación:\")\n",
    "    print(f\"Imagen: {test_image}\")\n",
    "    print(f\"Categoría: {category}\")\n",
    "    print(f\"Marca: {brand}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "0cd9f15b-fdd8-44c4-98cc-a2902426bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Verificando/descargando llava:13b...\n",
      "\n",
      "Procesando dataset en: dataset/train/images\n",
      "Dataset procesado y características extraídas\n",
      "\n",
      "Resultado de clasificación para ej4.jpeg:\n",
      "\n",
      "Productos detectados:\n",
      "\n",
      "Categoría: milk-powder\n",
      "Marca: leche entera\n",
      "Cantidad: 4 unidades\n",
      "\n",
      "Categoría: cereals\n",
      "Marca: corn flakes\n",
      "Cantidad: 1 unidades\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Union\n",
    "import requests\n",
    "import base64\n",
    "from PIL import Image\n",
    "import io\n",
    "import re\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class EnhancedVisionClassifier:\n",
    "    def __init__(self):\n",
    "        self.BASE_URL = \"http://localhost:11434/api/generate\"\n",
    "        self.MODEL_NAME = \"llava\"\n",
    "        self.categories = {\n",
    "            'biscuits': {\n",
    "                'brands': ['Casino', 'Club Social', 'Oreo', 'Ritz', 'Costa', 'Emperador', 'FRAC', 'Guarina'],\n",
    "                'keywords': ['galleta', 'cookie', 'cracker', 'wafer', 'biscuit', 'snack']\n",
    "            },\n",
    "            'canned': {\n",
    "                'brands': ['La Famosa', 'Goya', 'Del Monte'],\n",
    "                'keywords': ['enlatado', 'conserva', 'lata', 'atún', 'sardina']\n",
    "            },\n",
    "            'cereals': {\n",
    "                'brands': ['Nesquik', 'Corn Flakes', 'Trix', 'Gran Cereal'],\n",
    "                'keywords': ['cereal', 'avena', 'corn', 'granola', 'harina']\n",
    "            },\n",
    "            'dried-foods': {\n",
    "                'brands': ['Nesquik', 'Sustagen'],\n",
    "                'keywords': ['polvo', 'deshidratado', 'instantáneo']\n",
    "            },\n",
    "            'milk-powder': {\n",
    "                'brands': ['Milex', 'Nido', 'Carnation'],\n",
    "                'keywords': ['leche', 'milk', 'lácteo', 'fórmula']\n",
    "            },\n",
    "            'water': {\n",
    "                'brands': ['Planeta Azul', 'Dasani', 'Cascada'],\n",
    "                'keywords': ['agua', 'water', 'bebida', 'botella']\n",
    "            }\n",
    "        }\n",
    "        self.dataset_knowledge = {}\n",
    "        self.product_features = {}\n",
    "        \n",
    "    def load_and_process_dataset(self, dataset_path: str):\n",
    "        \"\"\"Carga y procesa el dataset, extrayendo características visuales de las imágenes\"\"\"\n",
    "        print(f\"\\nProcesando dataset en: {dataset_path}\")\n",
    "        \n",
    "        for category in self.categories.keys():\n",
    "            category_path = Path(dataset_path) / category\n",
    "            if not category_path.exists():\n",
    "                continue\n",
    "                \n",
    "            self.product_features[category] = []\n",
    "            for img_file in category_path.glob('*.[jJ][pP][gG]'):\n",
    "                if 'WhatsApp' not in img_file.name:\n",
    "                    try:\n",
    "                        # Extraer características visuales\n",
    "                        img = cv2.imread(str(img_file))\n",
    "                        if img is None:\n",
    "                            continue\n",
    "                            \n",
    "                        # Convertir a RGB y redimensionar\n",
    "                        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "                        img = cv2.resize(img, (224, 224))\n",
    "                        \n",
    "                        # Extraer características básicas\n",
    "                        features = {\n",
    "                            'name': img_file.stem,\n",
    "                            'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                            'dominant_colors': self._get_dominant_colors(img),\n",
    "                            'edges': self._get_edge_density(img)\n",
    "                        }\n",
    "                        \n",
    "                        self.product_features[category].append(features)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error procesando {img_file}: {e}\")\n",
    "                        \n",
    "        print(\"Dataset procesado y características extraídas\")\n",
    "\n",
    "    def _get_dominant_colors(self, img, n_colors=5):\n",
    "        \"\"\"Extrae los colores dominantes de una imagen\"\"\"\n",
    "        pixels = img.reshape(-1, 3)\n",
    "        pixels = np.float32(pixels)\n",
    "        \n",
    "        criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, .1)\n",
    "        flags = cv2.KMEANS_RANDOM_CENTERS\n",
    "        \n",
    "        _, labels, centers = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)\n",
    "        \n",
    "        _, counts = np.unique(labels, return_counts=True)\n",
    "        return centers[np.argsort(-counts)]\n",
    "        \n",
    "    def _get_edge_density(self, img):\n",
    "        \"\"\"Calcula la densidad de bordes en la imagen\"\"\"\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        return np.mean(edges > 0)\n",
    "\n",
    "    def generate_enhanced_prompt(self, image_path: str, image_features: Dict) -> str:\n",
    "        \"\"\"Genera un prompt mejorado basado en características de la imagen\"\"\"\n",
    "        base_prompt = (\n",
    "            f\"Analiza detalladamente esta imagen y enumera TODOS los productos visibles. \"\n",
    "            f\"IMPORTANTE: Si hay múltiples unidades del mismo producto, cuéntalas. \"\n",
    "            f\"Si hay productos diferentes en la misma imagen, enuméralos por separado.\\n\\n\"\n",
    "            f\"Para cada producto que veas, necesito:\\n\"\n",
    "            f\"1. Categoría exacta (elige de la lista proporcionada)\\n\"\n",
    "            f\"2. Marca específica\\n\"\n",
    "            f\"3. Cantidad precisa de unidades\\n\\n\"\n",
    "            f\"Para la categoría milk-powder, busca específicamente:\\n\"\n",
    "            f\"- Cajas o envases de leche\\n\"\n",
    "            f\"- Leche en polvo o líquida\\n\"\n",
    "            f\"- Marcas como Milex, Nido, Carnation o Leche Entera\\n\\n\"\n",
    "            f\"Los productos deben clasificarse en una de estas categorías:\\n\\n\"\n",
    "        )\n",
    "        \n",
    "        # Agregar información específica de cada categoría\n",
    "        for category, info in self.categories.items():\n",
    "            base_prompt += f\"- {category.upper()}: {', '.join(info['brands'])}\\n\"\n",
    "            base_prompt += f\"  Características típicas: {', '.join(info['keywords'])}\\n\"\n",
    "        \n",
    "        # Agregar información sobre características visuales detectadas\n",
    "        base_prompt += \"\\nCaracterísticas detectadas en la imagen:\\n\"\n",
    "        base_prompt += f\"- Densidad de bordes: {'alta' if image_features['edges'] > 0.1 else 'baja'}\\n\"\n",
    "        base_prompt += f\"- Colores dominantes: {len(image_features['dominant_colors'])} colores principales\\n\"\n",
    "        \n",
    "        base_prompt += \"\\nResponde en formato JSON como este ejemplo:\\n\"\n",
    "        base_prompt += \"\"\"[\n",
    "            {\"category\": \"milk-powder\", \"brand\": \"Leche Entera\", \"quantity\": 4},\n",
    "            {\"category\": \"cereals\", \"brand\": \"Corn Flakes\", \"quantity\": 1}\n",
    "        ]\"\"\"\n",
    "        return base_prompt\n",
    "\n",
    "    def classify_image(self, image_path: str) -> List[Dict[str, Union[str, int]]]:\n",
    "        \"\"\"Clasifica múltiples productos en una imagen y cuenta cantidades\n",
    "        \n",
    "        Returns:\n",
    "            List[Dict]: Lista de productos detectados con su categoría, marca y cantidad\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Cargar y procesar la imagen\n",
    "            img = cv2.imread(image_path)\n",
    "            if img is None:\n",
    "                return \"error\", \"error (imagen no válida)\"\n",
    "                \n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            img = cv2.resize(img, (224, 224))\n",
    "            \n",
    "            # Extraer características\n",
    "            image_features = {\n",
    "                'hist': cv2.calcHist([img], [0, 1, 2], None, [8, 8, 8], [0, 256, 0, 256, 0, 256]).flatten(),\n",
    "                'dominant_colors': self._get_dominant_colors(img),\n",
    "                'edges': self._get_edge_density(img)\n",
    "            }\n",
    "            \n",
    "            # Generar prompt mejorado\n",
    "            prompt = self.generate_enhanced_prompt(image_path, image_features)\n",
    "            \n",
    "            # Codificar imagen\n",
    "            with open(image_path, 'rb') as img_file:\n",
    "                base64_image = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "            \n",
    "            # Preparar payload\n",
    "            payload = {\n",
    "                \"model\": self.MODEL_NAME,\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"images\": [base64_image]\n",
    "            }\n",
    "            \n",
    "            # Realizar petición\n",
    "            response = requests.post(self.BASE_URL, json=payload)\n",
    "            if response.status_code != 200:\n",
    "                return \"error\", f\"error (código {response.status_code})\"\n",
    "                \n",
    "            result = response.json()\n",
    "            prediction = result.get('response', '').lower()\n",
    "            \n",
    "            # Procesar predicción\n",
    "            products = []\n",
    "            \n",
    "            # Primero intentar encontrar un formato JSON en la respuesta\n",
    "            json_matches = re.findall(r'\\[.*\\]', prediction.replace('\\n', ' '))\n",
    "            if json_matches:\n",
    "                try:\n",
    "                    products = json.loads(json_matches[0])\n",
    "                    if isinstance(products, list):\n",
    "                        return products\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "                    \n",
    "            # Si no se encontró JSON válido, procesar el texto\n",
    "            print(\"Procesando respuesta en texto plano:\", prediction)\n",
    "            # Procesamiento mejorado del texto\n",
    "            found_products = {}\n",
    "            \n",
    "            # Patrones de cantidad más específicos\n",
    "            quantity_patterns = [\n",
    "                r'(\\d+)\\s*(?:unidades?|cajas?|paquetes?|botellas?|envases?)',\n",
    "                r'hay\\s*(\\d+)',\n",
    "                r'se ven\\s*(\\d+)',\n",
    "                r'(\\d+)\\s*productos?',\n",
    "                r'cantidad[:\\s]+(\\d+)',\n",
    "                r'(\\d+)\\s*(?:milk|leche|agua|galletas?|cereales?)'\n",
    "            ]\n",
    "            \n",
    "            # Procesar por categoría\n",
    "            for category, info in self.categories.items():\n",
    "                category_keywords = set(info['keywords'])\n",
    "                category_brands = set(brand.lower() for brand in info['brands'])\n",
    "                \n",
    "                # Buscar coincidencias de categoría\n",
    "                if any(keyword in prediction.lower() for keyword in category_keywords):\n",
    "                    # Buscar marcas específicas\n",
    "                    for brand in info['brands']:\n",
    "                        brand_lower = brand.lower()\n",
    "                        if brand_lower in prediction.lower():\n",
    "                            product_key = f\"{category}-{brand}\"\n",
    "                            if product_key not in found_products:\n",
    "                                # Buscar cantidad usando todos los patrones\n",
    "                                quantities = []\n",
    "                                for pattern in quantity_patterns:\n",
    "                                    matches = re.findall(pattern, prediction.lower())\n",
    "                                    quantities.extend(matches)\n",
    "                                \n",
    "                                # Si encontramos cantidades, usar la más grande\n",
    "                                quantity = 1\n",
    "                                if quantities:\n",
    "                                    quantity = max(int(q) for q in quantities if q.isdigit())\n",
    "                                \n",
    "                                products.append({\n",
    "                                    \"category\": category,\n",
    "                                    \"brand\": brand,\n",
    "                                    \"quantity\": quantity\n",
    "                                })\n",
    "                                found_products[product_key] = True\n",
    "                                \n",
    "                                print(f\"Encontrado: {category} - {brand} - {quantity} unidades\")\n",
    "            \n",
    "            if not products:\n",
    "                products.append({\n",
    "                    \"category\": \"desconocido\",\n",
    "                    \"brand\": \"marca desconocida\",\n",
    "                    \"quantity\": 1\n",
    "                })\n",
    "            \n",
    "            return products\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error en clasificación: {e}\")\n",
    "            return \"error\", f\"error ({str(e)})\"\n",
    "\n",
    "    def batch_classify(self, image_paths: List[str]) -> List[Tuple[str, str]]:\n",
    "        \"\"\"Clasifica un lote de imágenes\"\"\"\n",
    "        results = []\n",
    "        for path in image_paths:\n",
    "            category, brand = self.classify_image(path)\n",
    "            results.append((path, category, brand))\n",
    "        return results\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    classifier = EnhancedVisionClassifier()\n",
    "    \n",
    "    try:\n",
    "        # Verificar que Ollama está funcionando\n",
    "        response = requests.get(\"http://localhost:11434/api/tags\")\n",
    "        if response.status_code != 200:\n",
    "            print(\"Error: No se puede conectar con Ollama. Asegúrate de que esté corriendo.\")\n",
    "            exit(1)\n",
    "            \n",
    "        # Intentar descargar llava:13b si no está disponible\n",
    "        print(\"\\nVerificando/descargando llava:13b...\")\n",
    "        download_response = requests.post(\"http://localhost:11434/api/pull\", json={\"name\": \"llava:13b\"})\n",
    "        if download_response.status_code != 200:\n",
    "            print(\"Error descargando llava:13b. Por favor, ejecuta 'ollama pull llava:13b' manualmente\")\n",
    "            exit(1)\n",
    "    except requests.exceptions.ConnectionError:\n",
    "        print(\"Error: No se puede conectar con Ollama. ¿Está corriendo el servicio?\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Configurar y cargar dataset\n",
    "    dataset_path = \"dataset/train/images\"\n",
    "    classifier.load_and_process_dataset(dataset_path)\n",
    "    \n",
    "    # Ejemplo de clasificación\n",
    "    test_image = \"ej4.jpeg\"\n",
    "    products = classifier.classify_image(test_image)\n",
    "    \n",
    "    print(f\"\\nResultado de clasificación para {test_image}:\")\n",
    "    print(\"\\nProductos detectados:\")\n",
    "    \n",
    "    if isinstance(products, list):\n",
    "        for product in products:\n",
    "            if isinstance(product, dict):\n",
    "                print(f\"\\nCategoría: {product.get('category', 'desconocido')}\")\n",
    "                print(f\"Marca: {product.get('brand', 'desconocida')}\")\n",
    "                print(f\"Cantidad: {product.get('quantity', 1)} unidades\")\n",
    "            else:\n",
    "                print(f\"Error: Formato de producto inesperado - {product}\")\n",
    "    else:\n",
    "        print(\"Error en la clasificación:\", products)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6947fc3-18df-49e9-909e-b8a26834218e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
